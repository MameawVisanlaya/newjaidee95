import os
import json
import requests
import streamlit as st
from google.cloud import storage
from dotenv import load_dotenv
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤ Environment Variables ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .env
load_dotenv()

# ‡∏î‡∏∂‡∏á API Key ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .env
API_KEY = os.getenv("API_KEY")

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Google Cloud Service Account
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/workspaces/newjaidee95/sasichatbot59-3461e68bb98f.json"

# Google Cloud Storage Bucket
BUCKET_NAME = "chat_bot_file"
FILE_PATH = "Chatbot/Chunks/ocr_results_chunks.jsonl"

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Google Cloud Storage
def load_chunks_from_gcs(bucket_name, file_path):
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(file_path)
        content = blob.download_as_text()
        chunks = [json.loads(line) for line in content.splitlines()]
        return chunks
    except Exception as e:
        st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Google Cloud Storage ‡πÑ‡∏î‡πâ: {e}")
        return []

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏î‡πâ‡∏ß‡∏¢ Text Embedding 004
def get_embedding(text):
    url = "https://generativelanguage.googleapis.com/v1beta/models/textembedding-gecko-004:embedText"
    headers = {"Content-Type": "application/json"}
    payload = {"text": text}
    try:
        response = requests.post(f"{url}?key={API_KEY}", json=payload, headers=headers)
        response_data = response.json()
        if "embedding" in response_data:
            return response_data["embedding"]["value"]
        else:
            return None
    except Exception as e:
        st.error(f"Error generating embedding: {e}")
        return None

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
def get_relevant_context(question, chunks):
    question_embedding = get_embedding(question)
    if question_embedding is None:
        return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏∞"

    context_embeddings = []
    context_texts = []

    for chunk in chunks:
        text = chunk.get("text", "")
        embedding = get_embedding(text)
        if embedding is not None:
            context_embeddings.append(embedding)
            context_texts.append(text)

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á (Cosine Similarity)
    similarities = cosine_similarity([question_embedding], context_embeddings)[0]

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 3 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    top_indices = np.argsort(similarities)[::-1][:3]
    top_contexts = [context_texts[i] for i in top_indices]

    # ‡∏õ‡∏£‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÉ‡∏´‡πâ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏î‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏û‡∏ó‡∏¢‡πå
    return " ".join(top_contexts) + " ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏±‡∏á‡∏ß‡∏• ‡∏Ñ‡∏ß‡∏£‡∏ô‡∏±‡∏î‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏∞"

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ Generative AI API
def generate_answer_with_gemini(question, context="", chat_history=[]):
    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"
    headers = {"Content-Type": "application/json"}
    
    # ‡∏£‡∏ß‡∏°‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏ô Prompt
    history_text = "\n".join(
        [f"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {entry[1]}" if entry[0] == "user" else f"‡∏ô‡πâ‡∏≠‡∏á‡πÉ‡∏à‡∏î‡∏µ: {entry[1]}" 
         for entry in chat_history]
    )

    prompt = f"""
        ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó: ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å ‡πÉ‡∏à‡∏î‡∏µ ‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ü‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à  ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏î‡πâ‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡πÇ‡∏†‡∏ä‡∏ô‡∏≤‡∏Å‡∏≤‡∏£ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏±‡∏á‡∏ß‡∏•

        ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:
        - ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
        - ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 3-4 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        - ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à
        - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ô‡∏±‡∏î‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏™‡∏á‡∏™‡∏±‡∏¢

        ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó:
        {context}

        ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:
        {history_text}

        ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà: {question}

        ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡πâ‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡πÄ‡∏ä‡πà‡∏ô ‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏Ñ‡πà‡∏∞" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏ô‡∏∞‡∏Ñ‡∏∞"
    """
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generation_config": {
            "max_output_tokens": 350,
            "temperature": 0.3,
            "top_p": 0.8,
            "top_k": 40
        }
    }
    try:
        response = requests.post(f"{url}?key={API_KEY}", json=payload, headers=headers)
        response_data = response.json()
        if "candidates" in response_data:
            return response_data["candidates"][0]["content"]["parts"][0]["text"]
        else:
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡πà‡∏∞"
    except Exception as e:
        return f"Error: {e}"

# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Streamlit UI
st.title("üòä ‡∏ô‡πâ‡∏≠‡∏á‡πÉ‡∏à‡∏î‡∏µ")
st.markdown("üë©‚Äç‚öïÔ∏èü©∫üí¨ ‡∏ô‡πâ‡∏≠‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏ü‡∏±‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏à‡∏¥‡∏ï‡πÉ‡∏à ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏î‡πâ‡∏≤‡∏ô‡πÇ‡∏†‡∏ä‡∏ô‡∏≤‡∏Å‡∏≤‡∏£")

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á session_state
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Chunk ‡∏à‡∏≤‡∏Å Google Cloud Storage
st.write("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Google Cloud Storage...")
chunks = load_chunks_from_gcs(BUCKET_NAME, FILE_PATH)
if chunks:
    st.success("‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")

# ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
if user_input := st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà..."):

    # ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á 
    combined_context = get_relevant_context(user_input, chunks)  

    # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    if os.path.exists('chat_history.json'):
        with open('chat_history.json', 'r', encoding='utf-8') as f:
            try:
                chat_history = json.load(f)
            except json.JSONDecodeError:  # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                chat_history = []
    else:
        chat_history = []

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô list
    chat_history.append(("user", user_input))

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    bot_response = generate_answer_with_gemini(  # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        user_input, 
        context=combined_context, 
        chat_history=st.session_state["chat_history"]
    )

    chat_history.append(("assistant", bot_response))  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á bot 

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å chat_history ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON
    with open('chat_history.json', 'w', encoding='utf-8') as f:
        json.dump(chat_history, f, ensure_ascii=False, indent=4)

    st.session_state["chat_history"].append(("user", user_input))
    st.chat_message("user").markdown(user_input)

    st.chat_message("assistant").markdown(bot_response)


if st.button("üîÑ ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"):
    if st.session_state["chat_history"]:
        st.session_state["chat_history"].pop()  # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°/‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
